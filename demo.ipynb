{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\RAG_test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from SGG_Benchmark_main.demo.demo_model import SGG_Model\n",
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_pre(source):\n",
    "\n",
    "    config_path = \"output_motifs/config.yml\"\n",
    "    dict_path = \"VG_dicts.json\"\n",
    "    weights_path = \"output_motifs/best_model_epoch_4.pth\"\n",
    "\n",
    "    example_img = source  \n",
    "\n",
    "    img = cv2.imread(example_img)\n",
    "    img=cv2.resize(img,(1024,1024))\n",
    "\n",
    "    model = SGG_Model(config_path, dict_path, weights_path, rel_conf=0.01, box_conf=0.1, show_fps=False)\n",
    "\n",
    "    predictions = model.predict(img)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def SGG(img_path):\n",
    "    \"\"\"SGG, input image path, return processed result\"\"\"\n",
    "\n",
    "    predictions=get_pre(img_path)\n",
    "\n",
    "    predictions=deal_SGG_result(predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_pos(bbox,orig_size=[1024,1024]):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        bbox xywh\n",
    "        orig_size  x,y \n",
    "    outputs:\n",
    "        Position of the box in the image\n",
    "        top left corner, top, top right corner\n",
    "        left, center, right\n",
    "        lower left corner, lower,lower right corner\n",
    "        012\n",
    "        345\n",
    "        678\n",
    "           \n",
    "    \"\"\"\n",
    "    center=((bbox[0]+bbox[2])/2,(bbox[1]+bbox[3])/2)    #\n",
    "    #center=(bbox[0],bbox[1])\n",
    "\n",
    "    if center[0]<orig_size[0]/3 and center[1]<orig_size[1]/3:\n",
    "        return 'top left corner'\n",
    "    if center[0]>=orig_size[0]/3 and center[0]<=2*orig_size[0]/3 and center[1]<orig_size[1]/3:\n",
    "        return 'top'\n",
    "    if center[0]>2*orig_size[0]/3 and center[1]<orig_size[1]/3:\n",
    "        return 'top right corner'\n",
    "    if center[0]<orig_size[0]/3 and center[1]>=orig_size[1]/3 and center[1]<=2*orig_size[1]/3:\n",
    "        return 'left'\n",
    "    if center[0]>=orig_size[0]/3 and center[0]<=2*orig_size[0]/3 and center[1]>=orig_size[1]/3 and center[1]<=2*orig_size[1]/3:\n",
    "        return 'center'\n",
    "    if center[0]>2*orig_size[0]/3 and center[1]>=orig_size[1]/3 and center[1]<=2*orig_size[1]/3:\n",
    "        return 'right'\n",
    "    if center[0]<orig_size[0]/3 and center[1]>2*orig_size[1]/3:\n",
    "        return 'lower left corner'\n",
    "    if center[0]>=orig_size[0]/3 and center[0]<=2*orig_size[0]/3 and center[1]>2*orig_size[1]/3:\n",
    "        return 'lower'\n",
    "    if center[0]>2*orig_size[0]/3 and center[1]>2*orig_size[1]/3:\n",
    "        return 'lower right corner'\n",
    "\n",
    "\n",
    "def fix(predictions,probs=0.15):\n",
    "    fix=np.array(predictions['bbox_scores'])>probs\n",
    "\n",
    "    bbox=np.array(predictions['bbox'])[fix]\n",
    "    bbox_labels=np.array(predictions['bbox_labels'])[fix]\n",
    "\n",
    "    bbox_idx=bbox_labels.shape[0]\n",
    "\n",
    "    rel_pairs=[]\n",
    "    rel_labels=[]\n",
    "\n",
    "    for i,pair in enumerate(predictions['rel_pairs'][:50]):\n",
    "        if pair[0]<bbox_idx and pair[1]<bbox_idx:\n",
    "            rel_pairs.append(pair)\n",
    "            rel_labels.append(predictions['rel_labels'][i])\n",
    "\n",
    "    return list(bbox),list(bbox_labels),rel_pairs,rel_labels\n",
    "\n",
    "def deal_SGG_result(predictions):\n",
    "\n",
    "\n",
    "    bbox,bbox_labels,rel_pairs,rel_labels=fix(predictions)\n",
    "\n",
    "    \n",
    "    object_count={}\n",
    "    #for i in predictions['bbox_labels']:\n",
    "    for i in bbox_labels:\n",
    "        if i in object_count.keys():\n",
    "            object_count[i]+=1\n",
    "        else:\n",
    "            object_count[i]=1\n",
    "\n",
    "    location_dict={key:[] for key in object_count.keys()}\n",
    "\n",
    "    #for i in range(len(predictions['bbox'])):\n",
    "    for i in range(len(bbox)):\n",
    "        #location_dict[predictions['bbox_labels'][i]].append(get_box_pos(predictions['bbox'][i],shape))\n",
    "        location_dict[bbox_labels[i]].append(get_box_pos(bbox[i]))\n",
    "\n",
    "\n",
    "    relation_dict=[]\n",
    "\n",
    "    #for i in range(len(predictions['rel_pairs'])):\n",
    "    #    relation_dict.append([predictions['bbox_labels'][predictions['rel_pairs'][i][0]],predictions['rel_labels'][i],predictions['bbox_labels'][predictions['rel_pairs'][i][1]]])\n",
    "    for i in range(len(rel_pairs)):\n",
    "        relation_dict.append([bbox_labels[rel_pairs[i][0]],rel_labels[i],bbox_labels[rel_pairs[i][1]]])\n",
    "\n",
    "    deal_result={\"number\":object_count,\"location\":location_dict,\"relationship\":relation_dict}\n",
    "    \n",
    "    block=[]\n",
    "\n",
    "    for i in deal_result['number'].keys():            \n",
    "        dicts={i:{'number':deal_result['number'][i],'location':deal_result['location'][i]}}\n",
    "\n",
    "        rel_list=[]\n",
    "        for j in deal_result['relationship']:\n",
    "            if j[0]==i or j[2]==i:\n",
    "                rel_list.append(j)\n",
    "\n",
    "        dicts[i]['relationship']=rel_list\n",
    "        block.append(dicts)\n",
    "\n",
    "\n",
    "    return block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\RAG_test\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def get_embed(sentences,model,tokenizer):\n",
    "    \"\"\"Calculate sentence encoding\"\"\"\n",
    "\n",
    "\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_simi(vector,vector_list):\n",
    "    \"\"\"Calculate the cosine similarity between the question and SGG\n",
    "    Vector: Question corresponds to embedded vector\n",
    "    vector_list: The result of SGG\n",
    "    \"\"\"\n",
    "    return F.cosine_similarity(vector,vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_part(query,model,tokenizer):\n",
    "    \"\"\"\n",
    "    Receive user questions and convert them into embeddings\n",
    "    \"\"\"\n",
    "    return get_embed(query,model,tokenizer)\n",
    "\n",
    "def img_part(img_path,model,tokenizer):\n",
    "\n",
    "    SGG_result=SGG(img_path)\n",
    "\n",
    "    sgg_embed=get_embed([list(i.keys())[0] for i in SGG_result],model,tokenizer)\n",
    "\n",
    "    return SGG_result,sgg_embed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(query_embed,SGG_result,SGG_embed):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant information to the query,\n",
    "    \"\"\"\n",
    "    simi=compute_cosine_simi(query_embed,SGG_embed)\n",
    "    sort_simi,sort_index=torch.sort(simi,descending=True)\n",
    "\n",
    "    if len(sort_index)<4:\n",
    "        return [SGG_result[i] for i in sort_index]\n",
    "    else:\n",
    "        return [SGG_result[i] for i in sort_index[:4]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(query,retrieval_result):\n",
    "    \"\"\"\n",
    "    get prompt\n",
    "    \"\"\"\n",
    "    prompt=\"The key information is represented by a list, each item is a dictionary\\\n",
    " containing three items: number represents the quantity of the object, location\\\n",
    " represents the position of the object, and relationship is a list, each item represents its relationship with other objects\\\n",
    ". Now please follow the following requirements: Please integrate all the information provided to you and use logical reasoning to answer the question as briefly as possible. If the information provided to you is not relevant to the question, please answer 'I don't know',Pretend that the message you received is a picture\\\n",
    "and based on the information:{} in the picture,please answer the following\\\n",
    " questions:{}\".format(str(retrieval_result),query)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "def get_response(prompt):\n",
    "    client = OpenAI(\n",
    "        api_key=\"\", # add your keys\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", \n",
    "    )\n",
    "\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen-max\",\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are an assistant specialized in processing images,I will give you some information as image input, please pretend that you have received the image'},\n",
    "            {'role': 'user', 'content': prompt}],\n",
    "        seed=2\n",
    "        )\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accurate_VQA(query,img):\n",
    "    \"\"\"\n",
    "    input: query: Your Question\n",
    "    img: Your image path\n",
    "    \"\"\"\n",
    "    query_embed=query_part(query,model,tokenizer)\n",
    "    SGG_result,SGG_embed=img_part(img,model,tokenizer)\n",
    "    retrieval_result=retrieval(query_embed,SGG_result,SGG_embed)\n",
    "    prompt=get_prompt(query,retrieval_result)\n",
    "    response=get_response(prompt)\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors from .\\glove.6B.200d.pt\n",
      "loading word vectors from .\\glove.6B.200d.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\RAG_test\\lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There is 1 man in the picture.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accurate_VQA(\"How many man?\",'JPEGImages\\\\598.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
